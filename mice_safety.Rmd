---
title: "mice"
output: html_document
date: "2025-03-11"
---
### Mice imputation for ideology

Before continuing to the models we take a look at missing values in the dataset and the possibility to impute them. We did the descriptive analysis without previous imputation as we perceived that descriptive makes only sense with the original dataset and the real values we know to be true. 

First, we take a look at the NA distribution of the Eurobarometer dataset. 

```{r}
colSums(is.na(individual_data))
```

The variables with most missing values are ideology, with 4689 NAs, which supposes a 17% of missing data within the variable (5689/27438), and social_class, with 1021 NAs, making up 21% of the variable (5883/27438). Religion has 483 NAs and contact_lgbti has 494 NAs. Trans_name has 3280 NAs, but we will not impute it as it is our target variable. The variables marital_status and personal_satis have 68 and 102 NAs respectively. Due to the low number we can go ahead and simply remove the rows. We remove caseid as it could confuse our mice imputation. 

```{r}
mice_data <- individual_data %>% 
  drop_na(c(marital_status, personal_satis)) %>% 
  select(-c(caseid))

colSums(is.na(mice_data))

```
We are left with 4599 NAs for ideology, 472 NAs for contact_lgbti, 448 NAs for religion and 973 NAs for social class. 

First, we are going to manually look at the different imputation methods and strategies to understand how close they come to the original dataset. 
As there are factor and numerical variables, the best options to test tje imputation are Random Forest and Pmm methods.

```{r}
imputed_comparison <- data.frame(
  original = mice_data$ideology,
  imputed_rf = complete(mice(mice_data, method = "rf", 
                             m = 3, maxit = 3, 
                             seed = 123))$ideology,
  imputed_pmm = complete(mice(mice_data, method = "pmm",
                              m = 3, maxit = 3, 
                              seed = 123))$ideology)
```

The m and the maxit arguments are = 3 to reduce loading time without losing so much information.

The next step is to plot the results to compare the resulting distributions: 

```{r}
# Arguments for the plot 
methods <- c("original", "imputed_rf" , "imputed_pmm")
titles <- c("Distribution of the Age Variable",
           "Random Forest-imputed Distribution",
            "PMM-imputed Distribution")
colors_fill <- c( "#E7CB94", "#E7969C", "#DE9ED6")


# Long format 
data_imputed_long <- imputed_comparison |>
  pivot_longer(cols = all_of(methods), names_to = "method", 
                values_to = "value") |>
  mutate(title = factor(method, levels = methods, labels = titles))


# Distributions comparison 
plot_mice <- ggplot(data_imputed_long, aes(x = value, fill = title)) +
  geom_histogram(binwidth = 1, color = "black", position = "identity") +
  facet_wrap(~ title, scales = "free_y") +
  scale_fill_manual(values = colors_fill) +
  theme_classic() +
  theme(legend.position = "none")

plot_mice
```

TODO: short interpretation 

Now, we go ahead with the imputation of the complete dataset. We use m = 3, as our missing values range between 5% and 20% of the observations. Hence, m=3 is still powerful enough to have reasonable imputation. 

```{r}
init = mice(mice_data, m = 3, seed = 123)
```

For our target variable trans_name we set method="", so that it is not imputed. 

```{r}
meth = init$method

meth

meth[c("trans_name")]=""

imputed_data = mice(mice_data, method=meth, m=3, seed=123)
```

```{r}
summary(imputed_data)
```

# Data set selection 

For modeling, the best would be to use the function `with()` and all 3 generated datasets of mice, which would give us a better result but also take more time. Due to practicality and the point of the exercise not being focused on perfect mice usage, we take the decision of extracting and keep working with only one of the datasets. 

```{r}
imputed_data_complete <- complete(imputed_data)

write.csv(imputed_data_complete, "imputed_data_complete.csv")
# my_data <- read.csv("imputed_data_complete.csv")
```

# Merging datasets 

```{r}
data <- imputed_data_complete |> 
  left_join(rainbow, by = c("isocntry" = "country_iso")) |> 
  left_join(trans, by = c("isocntry" = "country_iso")) |> 
  left_join(economic_indicators, by = c("isocntry" = "country_iso")) |> 
  left_join(religiosity, by = c("isocntry" = "country_iso"))

```




# Final model 

```{r}
final_model <- glmer(trans_name ~ 
                       scale(age) + # Cuando incluyes tanto age como age^2, 
                       #el término lineal (age) y el término cuadrático (age^2) trabajan
                       # juntos para modelar una relación curvilínea, entonces aunque age no es significativa 
                       # no la vamos a eliminar
                       I(scale(age)^2) +
                       female + 
                       occupation +
                       religion +
                       personal_satis +
                       contact_lgbti*scale(rain_ind) +
                       self_determination +
                       scale(ideology)*scale(gdp_pc) +
                       (1  + scale(age) +
                          female +
                          scale(ideology)
                        |isocntry), 
                     family = binomial(link = "logit"),
                     control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 100000)),
                     data = imputed_data_final_2)

str(imputed_data_final_2)

summary(final_model)
```

```{r}
# Logistic prediction (no va)

set.seed(123)

wofi <- imputed_data_final_2 |> 
  filter(isocntry != "FI")

test_country <-imputed_data_final_2 |> 
  filter(isocntry == "FI")

train_set <- wofi %>%
  group_by(isocntry) %>%
  sample_frac(0.7) %>%
  ungroup()

test_set <- anti_join(wofi, train_set, by = c("serialid")) 

testing <- bind_rows(test_set, test_country)

train_set <- train_set |> 
  select(trans_name:religiosity_percent, isocntry)

test_set <- test_set |> 
  select(trans_name:religiosity_percent, isocntry)

library(caret)
library(DataExplorer)
plot_intro(imputed_data_final_2) 

imputed_data_final_3 <- imputed_data_final_2 %>% 
  filter(!is.na(trans_name))

write.csv(imputed_data_final_3, "imputed_data_final_3.csv")
# my_data <- read.csv("my_data.csv")

in_train <- createDataPartition(imputed_data_final_3$trans_name, p = 0.7, list = FALSE)  # 70% for training
training <- imputed_data_final_3[ in_train,]
testing <- imputed_data_final_3[-in_train,]

final_model <- glmer(trans_name ~ 
                       scale(age) + # Cuando incluyes tanto age como age^2, 
                       #el término lineal (age) y el término cuadrático (age^2) trabajan
                       # juntos para modelar una relación curvilínea, entonces aunque age no es significativa 
                       # no la vamos a eliminar
                       I(scale(age)^2) +
                       female + 
                       occupation + 
                       religion +  # Si la interacciín es significativa no debemos quitar las variables individuales aunque no lo sean
                       personal_satis +
                       contact_lgbti*scale(rain_ind) +
                       self_determination +
                       scale(ideology)*scale(gdp_pc) +
                       (1  + scale(age) +
                          female +
                          scale(ideology)
                        |isocntry), 
                     family = binomial(link = "logit"),
                     control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 100000)),
                     data = training)

summary(final_model)

train <- training[complete.cases(training), ]

test <- testing[complete.cases(testing), ]


colSums(is.na(testing))  # Check for missing values in testing


pred <- predict(final_model, newdata = test, type = "response")

prediction <- as.factor(ifelse(pred>0.5, 1, 0))
prediction <- factor(ifelse(pred > 0.5, 1, 0), levels = c(0, 1))

length(prediction)

str(test)
str(prediction)

attr(prediction, "names") <- NULL


levels(prediction)
levels(test$trans_name)


test$trans_name <- factor(test$trans_name, levels = c(0, 1))

confusionMatrix(prediction, test$trans_name)$table
confusionMatrix(prediction, test$trans_name)$overall[1:2]

str(x)

```

